# -*- coding: utf-8 -*-
"""SB_Inference_Model_v0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nq5svk557v6HKT3zfIUXuAJibDZUfquc
"""

# === 1. Imports ===
import torch
import json
import os

# === 2. Setup Directories (modify as needed) ===
USER_TO_PLAYLISTS_PATH = "/content/drive/MyDrive/datasets/main_datasets/user_playlist_match/user_to_neighboursPlaylists.pkl"
MODEL_WEIGHTS_PATH = "/content/drive/MyDrive/models/model.pth"
MODEL_CONFIG_PATH = "/content/drive/MyDrive/models/model_config.json"

# === 3. Load model config ===
with open(MODEL_CONFIG_PATH, "r") as f:
    model_config = json.load(f)

# === 4. Load full model ===
model = torch.load(MODEL_WEIGHTS_PATH, map_location="cpu", weights_only=False)
model.eval()

# === 5. Attach scoring function for inference ===
def score(user_ids, playlist_ids):
    u = model.user_embeddings(user_ids)
    p = model.playlist_embeddings(playlist_ids)
    return (u * p).sum(1)

model.score = score

# === 6. Load user-to-playlists mapping ===
with open(USER_TO_PLAYLISTS_PATH, "rb") as f:
    user_to_playlists = pickle.load(f)

# === 7. Ranking function (uses internal mapping) ===
def rank_playlists_for_user(user_id, top_k=10):
    candidate_playlists = user_to_playlists.get(user_id, [])
    if not candidate_playlists:
        return []
    user_tensor = torch.LongTensor([user_id] * len(candidate_playlists))
    playlist_tensor = torch.LongTensor(candidate_playlists)
    scores = model.score(user_tensor, playlist_tensor)
    top_indices = torch.topk(scores, k=min(top_k, len(candidate_playlists))).indices
    return [candidate_playlists[i] for i in top_indices]

# === 8. Example usage ===
example_user = 42
top_playlists = rank_playlists_for_user(example_user, top_k=5)
print(f"Top playlists for user {example_user}:", top_playlists)

import random
import time

# === 1. Select 100 random users from mapping ===
all_users = list(user_to_playlists.keys())
sample_users = random.sample(all_users, 100)

# === 2. Measure latency per inference ===
latencies = []

for user_id in sample_users:
    start = time.perf_counter()
    _ = rank_playlists_for_user(user_id, top_k=10)
    end = time.perf_counter()
    latencies.append((end - start) * 1000)  # ms

# === 3. Report results ===
avg_latency = sum(latencies) / len(latencies)
print(f"Inference latency over {len(latencies)} users:")
print(f"- Average: {avg_latency:.2f} ms")
print(f"- Min: {min(latencies):.2f} ms")
print(f"- Max: {max(latencies):.2f} ms")