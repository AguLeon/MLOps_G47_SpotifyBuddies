# -*- coding: utf-8 -*-
"""SB_Inference_Model_v0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nq5svk557v6HKT3zfIUXuAJibDZUfquc
"""

# === 1. Imports ===
import torch
import json
import os
import pickle

# === 2. Setup Directories ===
USER_TO_PLAYLISTS_PATH = "/content/drive/MyDrive/datasets/main_datasets/user_playlist_match/user_to_neighboursPlaylists.pkl"
MODEL_WEIGHTS_PATH = "/content/drive/MyDrive/models/model.pth"
MODEL_CONFIG_PATH = "/content/drive/MyDrive/models/model_config.json"

# === 3. Load model config ===
with open(MODEL_CONFIG_PATH, "r") as f:
    model_config = json.load(f)

# === 4. Load full model ===
model = torch.load(MODEL_WEIGHTS_PATH, map_location="cpu", weights_only=False)
model.eval()

# === 5. Attach scoring function for inference ===
def score(user_ids, playlist_ids):
    u = model.user_embeddings(user_ids)
    p = model.playlist_embeddings(playlist_ids)
    return (u * p).sum(1)

model.score = score

# === 6. Load user-to-playlists mapping ===
with open(USER_TO_PLAYLISTS_PATH, "rb") as f:
    user_to_playlists = pickle.load(f)

# === 7. Ranking function (uses internal mapping) ===
def rank_playlists_for_user(user_id, top_k=10):
    candidate_playlists = user_to_playlists.get(user_id, [])
    if not candidate_playlists:
        return []
    user_tensor = torch.LongTensor([user_id] * len(candidate_playlists))
    playlist_tensor = torch.LongTensor(candidate_playlists)
    scores = model.score(user_tensor, playlist_tensor)
    top_indices = torch.topk(scores, k=min(top_k, len(candidate_playlists))).indices
    return [candidate_playlists[i] for i in top_indices]

# === 8. Example usage ===
example_user = 42
top_playlists = rank_playlists_for_user(example_user, top_k=10)
print(f"Top playlists for user {example_user}:", top_playlists)

# Define a batched inference function
def rank_playlists_for_users(user_ids: list[int], top_k=10):
    all_user_ids = []
    all_playlist_ids = []
    slice_bounds = []  # (user_id, start_idx, end_idx)

    # === 1. Prepare batched inputs ===
    for user_id in user_ids:
        playlists = user_to_playlists.get(user_id, [])
        if playlists:
            start = len(all_user_ids)
            all_user_ids.extend([user_id] * len(playlists))
            all_playlist_ids.extend(playlists)
            end = len(all_user_ids)
            slice_bounds.append((user_id, start, end))
        else:
            slice_bounds.append((user_id, None, None))  # No playlists

    if not all_user_ids:
        return {user_id: [] for user_id in user_ids}

    # === 2. Score all (user, playlist) pairs at once ===
    user_tensor = torch.LongTensor(all_user_ids)
    playlist_tensor = torch.LongTensor(all_playlist_ids)
    with torch.no_grad():
        scores = model.score(user_tensor, playlist_tensor)  # Shape: [total_pairs]

    # === 3. Rank top-k per user using precomputed slices ===
    result = {}
    for user_id, start, end in slice_bounds:
        if start is None:
            result[user_id] = []
        else:
            user_scores = scores[start:end]
            user_playlists = all_playlist_ids[start:end]
            topk = min(top_k, len(user_scores))
            top_indices = torch.topk(user_scores, k=topk).indices
            result[user_id] = [user_playlists[i] for i in top_indices]

    return result

## Example usage

test_user_ids = [42, 58, 90]

# Call the batched inference function
ranked_results = rank_playlists_for_users(test_user_ids, top_k=5)

# Print results
print("\n--- Batched Inference Results ---")
for user_id in test_user_ids:
    playlists = ranked_results.get(user_id, [])
    if playlists:
        print(f"User {user_id} → Top-{len(playlists)} Playlists: {playlists}")
    else:
        print(f"User {user_id} → No candidate playlists available.")

print (ranked_results)